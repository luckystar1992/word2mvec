#coding:utf-8

"""
è®­ç»ƒè¿‡ç¨‹
"""
import sys, os, argparse
import FileUtil
import Util
import Model
import math
import numpy as np
import time
import warnings
import tqdm
from multiprocessing import Value, Pool, Lock
from Huffman import Huffman

MAX_SEN_LEN = 1000              # å…è®¸æœ€é•¿å¥å­çš„å•è¯æ•°ç›®
SIGMOID_TABLE_SIZE = 1000       # sigmoidæŸ¥æ‰¾è¡¨çš„å¤§å°
SIGMOID_MAX_EXP = 6             # sigmoidçš„å·¦å³è¾¹ç•Œå¤§å°

class UnigramTable:

    def __init__(self, vocab):
        self.vocab = vocab
        self.vocab_size = len(vocab)
        self.power = 0.75
        self.norm = sum([math.pow(vocabItem.count, self.power) for vocabItem in vocab])

        self.table_size = int(1e8)
        self.table = np.zeros(self.table_size, dtype=np.uint32)

    def build(self):
        print("Build a UnigramTable")
        p = 0
        i = 0
        for index, unigram in tqdm.tqdm(enumerate(self.vocab)):
            p += float(math.pow(unigram.count, self.power)) / self.norm
            while i < self.table_size and float(i) / self.table_size < p:
                self.table[i] = index
                i += 1

    def sample(self, count):
        indices = np.random.randint(low=0, high=len(self.table), size=count)
        return [self.table[i] for i in indices]

class SigmoidTable:
    """æ„é€ sigmoidè¡¨ï¼Œè¿™æ ·æ¯æ¬¡åœ¨ä½¿ç”¨çš„æ—¶å€™å°±ç›´æ¥æŸ¥è¡¨ï¼Œä¸æ˜¯è®¡ç®—"""
    def build(self):
        self.table = np.zeros(SIGMOID_TABLE_SIZE)
        for i in range(SIGMOID_TABLE_SIZE):
            self.table[i] = math.exp((i / SIGMOID_TABLE_SIZE * 2 - 1) * SIGMOID_MAX_EXP)
            self.table[i] = self.table[i] / (self.table[i] + 1)


def init_process(*params):
    """çº¿ç¨‹çš„åˆå§‹åŒ–ï¼Œå°†å­çº¿ç¨‹éœ€è¦çš„å‚æ•°å…¨éƒ¨ä¼ é€’è¿‡æ¥"""
    global args, vocab, model, global_word_count, global_alpha, f_input
    args, vocab, model, global_word_count, global_alpha = params
    f_input = open(args.input, 'r')
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        model.net1 = np.ctypeslib.as_array(model.net1)
        model.net2 = np.ctypeslib.as_array(model.net2)


def train_process(pid):
    """å•ä¸ªçº¿ç¨‹çš„è®­ç»ƒ"""
    epoch = args.epoch_index
    start = args.start_list[pid]
    end = args.end_list[pid]
    alpha_coeff = 1 - (global_word_count.value + epoch * vocab.word_count)/(vocab.word_count * args.epoch + 1)
    # æ¯ä¸ªçº¿ç¨‹åªå¤„ç†corpusçš„æŸä¸€éƒ¨åˆ†
    f_input.seek(start)
    word_count = 0
    last_word_count = 0

    while f_input.tell() < end:
        # TODO: change readline to read one word each
        line = f_input.readline().strip()
        if not line:
            continue
        tokens = []
        tokens.append(FileUtil.BOL)
        tokens.extend(line.split())
        tokens.append(FileUtil.EOL)

        tokens = vocab.indices(tokens)

        # å¯¹å½“å‰çš„å¥å­è¿›è¡Œéå†
        for word_index, token in enumerate(tokens):

            # è¾“å‡ºè¿è¡Œè¿‡ç¨‹ä¿¡æ¯
            if global_word_count.value % int(vocab.word_count / 10000) == 0:
                # if pid == 0:
                #     sys.stdout.write("\r end-{end} current={current}".format(end=end, current=f_input.tell()))
                #     sys.stdout.flush()
                # è¾“å‡ºçº¿ç¨‹ä¿¡æ¯
                sys.stdout.write(
                    "\rğ‘¬-{epoch} ğœƒ(âº)={alpha_coeff:>4.2f} âº={alpha:>10.8f} ({current:>{len}d}/{total:>{len}d}){progress:>5.2f}Ùª".format(
                    epoch=epoch,
                    alpha_coeff=alpha_coeff,
                    alpha=global_alpha.value,
                    current=global_word_count.value,
                    len=len(str(vocab.word_count)),
                    total=vocab.word_count,
                    progress=float(global_word_count.value) / vocab.word_count * 100
                ))
                sys.stdout.flush()

            # æ›´æ–°alpha
            if word_count - last_word_count > 10000:
                last_word_count = word_count
                # alpah çš„è¡°å‡ç³»æ•°
                alpha_coeff = 1 - (global_word_count.value + epoch * vocab.word_count)/(vocab.word_count * args.epoch + 1)
                global_alpha.value = args.start_alpha * alpha_coeff
                if global_alpha.value < args.start_alpha * 0.0001:
                    global_alpha.value = args.start_alpha * 0.0001

            # éšæœºå–ä¸Šä¸‹æ–‡çª—å£å¤§å°
            current_window = np.random.randint(low=1, high=args.window_size + 1)
            context_start = max(word_index - current_window, 0)
            context_end = min(word_index + current_window + 1, len(tokens))
            context = tokens[context_start:word_index] + tokens[word_index + 1:context_end]

            # CBOWæ¨¡å‹
            if args.cbow:
                neu1 = np.mean(np.array([model.net1[c] for c in context]), axis=0)
                assert len(neu1) == args.embedding_size
                neu1e = np.zeros(args.embedding_size)
                if args.negative > 0:
                    classifiers = [(token, 1)] + [(target, 0) for target in args.table.sample(args.negative)]
                else:
                    classifiers = zip(vocab[token].path, vocab[token].code)

                for target, label in classifiers:
                    z = np.dot(neu1, model.net2[target])
                    if z <= -SIGMOID_MAX_EXP:
                        continue
                    elif z >= SIGMOID_MAX_EXP:
                        continue
                    else:
                        table_index = int((z + SIGMOID_MAX_EXP) * (SIGMOID_TABLE_SIZE / SIGMOID_MAX_EXP / 2))
                    p = args.sigmoid_table[table_index]
                    g = global_alpha.value * (1 - label - p)
                    neu1e += g * model.net2[target]
                    model.net2[target] += g * neu1

                # å‚æ•°æ›´æ–°
                for context_word in context:
                    model.net1[context_word] += neu1e
            # Skip-Gramæ¨¡å‹
            else:
                for context_word in context:
                    neu1e = np.zeros(args.embedding_size)
                    if args.negative > 0:
                        classifiers = [(token, 1)] + [(target, 0) for target in args.table.sample(args.negative)]
                    else:
                        classifiers = zip(vocab[token].path, vocab[token].code)

                    for target, label in classifiers:
                        z = np.dot(model.net1[context_word], model.net2[target])
                    if z <= -SIGMOID_MAX_EXP:
                        continue
                    elif z >= SIGMOID_MAX_EXP:
                        continue
                    else:
                        table_index = int((z + SIGMOID_MAX_EXP) * (SIGMOID_TABLE_SIZE / SIGMOID_MAX_EXP / 2))
                    p = args.sigmoid_table[table_index]
                    g = global_alpha.value * (1 - label - p)
                    neu1e += g * model.net2[target]
                    model.net2[target] += g * model.net1[context_word]

                model.net1[context_word] += neu1e


            word_count += 1
            global_word_count.value += 1

    sys.stdout.write(
        "\rğ‘¬-{epoch} ğœƒ(âº)={alpha_coeff:>4.2f} âº={alpha:>10.8f} ({current:>{len}d}/{total:>{len}d}){progress:>5.2f}Ùª".format(
            epoch=epoch,
            alpha_coeff=alpha_coeff,
            alpha=global_alpha.value,
            current=global_word_count.value,
            len=len(str(vocab.word_count)),
            total=vocab.word_count,
            progress=float(global_word_count.value) / vocab.word_count * 100
        ))
    sys.stdout.flush()

def multi_init_process(*params):
    """å¤šè¯­å¢ƒè¯å‘é‡æ¨¡å‹å¤šçº¿ç¨‹çš„åˆå§‹åŒ–"""
    global args, vocab, model, global_word_count, global_alpha, lock, f_input
    args, vocab, model, global_word_count, global_alpha, lock = params

    f_input = open(args.input, 'r')
    with warnings.catch_warnings():
        warnings.simplefilter('ignore', RuntimeWarning)
        model.senses_count = np.ctypeslib.as_array(model.senses_count)
        model.senses_access = np.ctypeslib.as_array(model.senses_access)
        model.main_embedding = np.ctypeslib.as_array(model.main_embedding)
        model.main_sense = np.ctypeslib.as_array(model.main_sense)
        model.embedding = np.ctypeslib.as_array(model.embedding)
        model.senses = np.ctypeslib.as_array(model.senses)
        model.weights = np.ctypeslib.as_array(model.weights)

def multi_train_process(pid):
    """å¤šè¯­å¢ƒè¯å‘é‡æ¨¡å‹çš„å•ä¸ªçº¿ç¨‹è®­ç»ƒ"""
    epoch = args.epoch_index
    start = args.start_list[pid]
    end = args.end_list[pid]
    alpha_coeff = 1 - (global_word_count.value + epoch * vocab.word_count) / (vocab.word_count * args.epoch + 1)
    # æ¯ä¸ªçº¿ç¨‹åªå¤„ç†corpusçš„æŸä¸€éƒ¨åˆ†
    f_input.seek(start)
    word_count = 0
    last_word_count = 0

    while f_input.tell() < end:
        line = f_input.readline().strip()
        if not line:
            continue
        sen_tokens = []
        sen_tokens.append(FileUtil.BOL)
        sen_tokens.extend(line.split())
        sen_tokens.append(FileUtil.EOL)
        tokens_id = vocab.indices(sen_tokens)

        # è®¡ç®—å‡ºæ•´ä¸ªå¥å­çš„tokenæ‰€åœ¨çš„index
        # å¦‚æœæ˜¯å‰é¢å‡ æ¬¡çš„è¯ï¼Œå…ˆè®­ç»ƒmain embedeing å’Œ main sense
        tokens_sense_index = [0] * len(tokens_id)
        for index, token_id in enumerate(tokens_id):
            # éšæœºå–ä¸Šä¸‹æ–‡çª—å£å¤§å°
            context_start = max(index - args.window_size, 0)
            # context_start = 0
            context_end = min(index + args.window_size + 1, len(tokens_id))
            # context_end = len(tokens_id)
            context_ids = tokens_id[context_start:index] + tokens_id[index + 1:context_end]

            """
            å¯¹äºå‡ºç°accessæ•°ç›®å’Œæœ€ç»ˆçš„frequentä¸ä¸€è‡´çš„æƒ…å†µï¼Œä¹Ÿèƒ½é€šè¿‡æ•°æ®é”è§£é‡Šçš„é€š
            """
            context_vector = model.getContextVector(context_ids)
            if model.senses_count[token_id] == 0:
                model.main_sense[token_id] = context_vector
                model.senses_count[token_id] = 1
                model.senses_access[token_id][0] = 1
            else:
                lock.acquire()
                cos_max_index, cos_max_value = model.getSimilarMax(context_vector, token_id)

                if cos_max_value > 0.8:
                    if cos_max_index == 0:
                        last_sense = model.main_sense[token_id]
                        last_access = model.senses_access[token_id][0]
                        model.main_sense[token_id] = (last_sense * last_access + context_vector) / (last_access + 1)
                    else:
                        # å°†å¯¹åº”çš„senseæ›´æ–°
                        last_sense = model.senses[token_id][cos_max_index - 1]
                        last_access = model.senses_access[token_id][cos_max_index - 1]
                        model.senses[token_id][cos_max_index - 1] = (last_sense * last_access + context_vector) / (last_access + 1)

                    model.senses_access[token_id][cos_max_index] += 1
                    tokens_sense_index[index] = cos_max_index

                else:
                    # æœªè¶…è¿‡sensesçš„å®¹é‡åˆ™æ–°å¢åŠ ä¸€ä¸ªsense
                    """
                    bug fix: éœ€è¦åœ¨è¿™é‡Œå®ç°ä¸€ä¸ªcountçš„æ•°æ®é”ï¼Œè¦ä¸ç„¶ä½¿ç”¨å¤šçº¿ç¨‹çš„æ—¶å€™
                             countæ¯æ¬¡åŠ 1çš„æ—¶å€™ï¼Œå¤šçº¿ç¨‹countå¯èƒ½å‡ºç°å¤šæ¬¡ï¼Œç„¶ååœ¨è®¡ç®—
                             getSimilarMaxçš„æ—¶å€™ï¼Œä¼šç”¨countç´¢å¼•è®¿é—®
                    """
                    if model.senses_count[token_id] < args.senses + 1:
                        count = model.senses_count[token_id]
                        model.senses[token_id][count - 1] = context_vector
                        tokens_sense_index[index] = count
                        model.senses_count[token_id] += 1
                        model.senses_access[token_id][count] = 1
                    # è¶…è¿‡å®¹é‡ï¼Œä½¿ç”¨main
                    else:
                        last_sense = model.main_sense[token_id]
                        last_access = model.senses_access[token_id][0]
                        model.main_sense[token_id] = (last_sense * last_access + context_vector) / (last_access + 1)
                        tokens_sense_index[index] = 0
                        model.senses_access[token_id][0] += 1
                lock.release()

        # å¯¹å½“å‰çš„å¥å­è¿›è¡Œéå†
        for word_index, token in enumerate(tokens_id):
            # è¾“å‡ºè¿è¡Œè¿‡ç¨‹ä¿¡æ¯
            #if global_word_count.value % int(vocab.word_count / 10000) == 0:
            sys.stdout.write(
                "\rğ‘¬-{epoch} ğœƒ(âº)={alpha_coeff:>4.2f} âº={alpha:>10.8f} ({current:>{len}d}/{total:>{len}d}){progress:>5.2f}Ùª".format(
                    epoch=epoch,
                    alpha_coeff=alpha_coeff,
                    alpha=global_alpha.value,
                    current=global_word_count.value,
                    len=len(str(vocab.word_count)),
                    total=vocab.word_count,
                    progress=float(global_word_count.value) / vocab.word_count * 100
                ))
            sys.stdout.flush()

            # æ›´æ–°alpha
            if word_count - last_word_count > 10000:
                last_word_count = word_count
                # alpah çš„è¡°å‡ç³»æ•°
                alpha_coeff = 1- (global_word_count.value + epoch * vocab.word_count) / (
                            vocab.word_count * args.epoch + 1)
                global_alpha.value = args.start_alpha * alpha_coeff
                if global_alpha.value < args.start_alpha * 0.0001:
                    global_alpha.value = args.start_alpha * 0.0001

            # éšæœºå–ä¸Šä¸‹æ–‡çª—å£å¤§å°
            rand_window = np.random.randint(low=1, high=args.window_size + 1)
            context_start = max(word_index - rand_window, 0)
            context_end = min(word_index + rand_window + 1, len(tokens_id))
            context_ids = tokens_id[context_start:word_index] + tokens_id[word_index + 1:context_end]
            # åŒæ—¶ä¹Ÿé€‰å–sense indexçš„list
            current_tokens_sense_index = tokens_sense_index[context_start:word_index] + tokens_sense_index[word_index + 1:context_end]
            context_vector = model.getContextVector(context_ids)

            # CBOWæ¨¡å‹
            if args.cbow:
                neu1e = np.zeros(args.embedding_size)
                if args.negative > 0:
                    classifiers = [(token, 1)] + [(target, 0) for target in args.table.sample(args.negative)]
                else:
                    classifiers = zip(vocab[token].path, vocab[token].code)

                for target, label in classifiers:
                    z = np.dot(context_vector, model.weights[target])
                    if z <= -SIGMOID_MAX_EXP:
                        continue
                    elif z >= SIGMOID_MAX_EXP:
                        continue
                    else:
                        table_index = int((z + SIGMOID_MAX_EXP) * (SIGMOID_TABLE_SIZE / SIGMOID_MAX_EXP / 2))
                    p = args.sigmoid_table[table_index]
                    g = global_alpha.value * (1 - label - p)
                    neu1e += g * model.weights[target]
                    model.weights[target] += g * context_vector

                # å‚æ•°æ›´æ–°
                for context_id, sense_index in zip(context_ids, current_tokens_sense_index):
                    if sense_index == 0:
                        model.main_embedding[context_id] += neu1e
                    else:
                        model.embedding[context_id][sense_index-1] += neu1e

            word_count += 1
            global_word_count.value += 1
    sys.stdout.write(
        "\rğ‘¬-{epoch} ğœƒ(âº)={alpha_coeff:>4.2f} âº={alpha:>10.8f} ({current:>{len}d}/{total:>{len}d}){progress:>5.2f}Ùª".format(
            epoch=epoch,
            alpha_coeff=alpha_coeff,
            alpha=global_alpha.value,
            current=global_word_count.value,
            len=len(str(vocab.word_count)),
            total=vocab.word_count,
            progress=float(global_word_count.value) / vocab.word_count * 100
        ))
    sys.stdout.flush()


def multi_train(args, vocab):
    if args.negative > 0:
        print("Initializing Unigram Table")
        args.table = UnigramTable(vocab)
        args.table.build()
    else:
        print("Initializing Huffman Tree")
        huffman = Huffman(vocab)
        huffman.encode()

    multiSenseModel = Model.MultiSenseModel(args, vocab)
    multiSenseModel.init_model()

    # å¼€å¯å¤šçº¿ç¨‹
    t0 = time.time()
    print("Begin Training with {0} threads.".format(args.num_threads))
    args.f_input = open(args.input)
    args.start_list, args.end_list = FileUtil.FileSplit().split(args, vocab)
    global_word_count = Value('i', 0)
    global_alpha = Value('f', args.alpha)
    lock = Lock()
    for epoch in range(0, args.epoch):
        t_begin = time.time()
        global_word_count.value = 0
        args.epoch_index = epoch
        pool = Pool(processes=args.num_threads,
                    initializer=multi_init_process,
                    initargs=(args, vocab, multiSenseModel, global_word_count, global_alpha, lock))
        pool.map(multi_train_process, range(args.num_threads))
        t_end = time.time()
        print("\rğ‘¬-{epoch} âº={alpha:>10.8f} ğ‘‡={time:>10.2f}min  token/ps {speed:>6.1f}".format(
            epoch=epoch,
            alpha=global_alpha.value,
            time=(t_end - t_begin)/60,
            speed=vocab.word_count/(t_end-t_begin)/args.num_threads
        ), end='')
        multiSenseModel.saveEmbedding(epoch)
    args.f_input.close()
    t1 = time.time()
    print("")
    print("Completed Training, Spend {spend_time:>10.2f} minutes.".format(spend_time=(t1-t0)/60))


def train(args, vocab):
    if args.negative > 0:
        print("Initializing Unigram Table")
        args.table = UnigramTable(vocab)
        args.table.build()
    else:
        print("Initializing Huffman Tree")
        huffman = Huffman(vocab)
        huffman.encode()

    singleModel = Model.SingleModel(args, vocab)
    singleModel.init_model()

    # å¼€å¯å¤šçº¿ç¨‹
    t0 = time.time()
    print("Begin Training with {0} threads.".format(args.num_threads))
    args.f_input = open(args.input)
    args.start_list, args.end_list = FileUtil.FileSplit().split(args, vocab)
    global_word_count = Value('i', 0)
    global_alpha = Value('f', args.alpha)
    for epoch in range(0, args.epoch):
        t_begin = time.time()
        global_word_count.value = 0
        args.epoch_index = epoch
        pool = Pool(processes=args.num_threads,
                    initializer=init_process,
                    initargs=(args, vocab, singleModel, global_word_count, global_alpha))
        pool.map(train_process, range(args.num_threads))
        t_end = time.time()
        print("\rğ‘¬-{epoch} âº={alpha:>10.8f} ğ‘‡={time:>10.2f}min  token/ps {speed:>6.1f}".format(
            epoch=epoch,
            alpha=global_alpha.value,
            time=(t_end - t_begin)/60,
            speed=vocab.word_count/(t_end-t_begin)/args.num_threads
        ), end='')
        singleModel.saveEmbedding(epoch)
    args.f_input.close()
    t1 = time.time()
    print("")
    print("Completed Training, Spend {spend_time:>10.2f} minutes.".format(spend_time=(t1-t0)/60))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-input', dest='input', required=True, help="è®­ç»ƒè¯­æ–™çš„è·¯å¾„")
    parser.add_argument('-cbow', dest='cbow', required=True, help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('-negative', dest='negative', type=int, required=True, help='æ±‚è§£æ¨¡å‹çš„åŠ é€Ÿç®—æ³•')
    parser.add_argument('-min_count', dest='min_count', type=int, help='è¯é¢‘æœ€å°è¦æ±‚å€¼')
    parser.add_argument('-epoch', dest='epoch',  type=int, default=5, help='è¯­æ–™å¾ªç¯æ¬¡æ•°')
    parser.add_argument('-embedding_size', dest='embedding_size', required=True, type=int, help='è¯å‘é‡å¤§å°')
    parser.add_argument('-window_size', dest='window_size', required=True, type=int, help='ä¸Šä¸‹æ–‡æœ€å¤§è·ç¦»')
    parser.add_argument('-num_threads', dest='num_threads', default=2, type=int, help='å¼€å¯çš„çº¿ç¨‹æ•°')
    parser.add_argument('-binary', dest='binary', default=1, type=int, help='äºŒè¿›åˆ¶ä¿å­˜è¯å‘é‡')
    parser.add_argument('-alpha', dest='alpha', default=0.025, type=float, help='åˆå§‹alphaå€¼')
    parser.add_argument('-out_folder', dest='out_folder', default='./out', help='æ¨¡å‹/å‘é‡ä¿å­˜æ–‡ä»¶å¤¹')
    parser.add_argument('-vocab_path', dest='vocab_path', required=False, help='å·²ç»å­˜åœ¨çš„è¯å…¸')
    parser.add_argument('-senses', dest='senses', required=False, type=int, help='è¯­å¢ƒæœ€å¤šæ¬¡æ•°')
    parser.add_argument('-senses_threshold', dest='tenses_threshold', type=int, default=-1, help='æ‹¥æœ‰å¤šè¯­å¢ƒtokençš„æœ€å°è¯é¢‘')

    args = parser.parse_args()
    args.start_alpha = args.alpha

    updateArgs = Util.UpdateArgs()
    updateArgs.update(args)

    vocab = FileUtil.Vocab(args)
    if hasattr(args, 'vocab_path') and args.vocab_path is not None:
        vocab_path = os.path.join(args.out_folder, args.vocab_path)
        if os.path.exists(vocab_path):
            vocab.loadFromFile(vocab_path)
        else:
            raise FileNotFoundError()
    else:
        vocab.build()
        input_name = 'vocab' + Util.getFileName(args.input) + ".txt"
        vocab_path = os.path.join(args.out_folder, input_name)
        vocab.save(vocab_path)

    sigmoidTable = SigmoidTable()
    sigmoidTable.build()
    args.sigmoid_table = sigmoidTable.table
    # æ­£å¼è®­ç»ƒ
    multi_train(args, vocab)

